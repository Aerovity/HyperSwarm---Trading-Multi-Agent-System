# RL Training Data Documentation

This directory contains all data generated by the Guardian Agent's RL training system.

## Directory Structure

```
data/
├── historical/              # Historical OHLCV data for training
│   └── btcusd_5m.parquet   # 5-minute BTC/USD candles
├── models/                  # Trained model files
│   ├── guardian_ppo_latest.zip
│   └── guardian_ppo_run_xxx.zip
├── training_runs/           # Training run data (see below)
│   └── run_YYYYMMDD_HHMMSS/
├── visualizations/          # Generated plots and reports
└── agent_logs.json          # Runtime agent logs
```

---

## Training Runs (`training_runs/`)

Each training run creates a timestamped folder with complete training data.

```
training_runs/
└── run_20250116_143022/
    ├── config.json              # Training configuration
    ├── metrics.json             # Episode summaries & checkpoints
    ├── detailed_episodes.jsonl  # Rich per-episode data
    ├── checkpoints/             # Model snapshots
    │   ├── model_step_50000.zip
    │   └── model_step_100000.zip
    └── tensorboard/             # TensorBoard logs
```

---

## Data Schemas

### `config.json` - Training Configuration

```json
{
    "run_id": "run_20250116_143022",
    "created_at": "2025-01-16T14:30:22.123456",
    "completed_at": "2025-01-16T16:45:33.789012",
    "status": "completed",
    "algorithm": "PPO",
    "total_timesteps": 500000,
    "learning_rate": 0.0003,
    "batch_size": 64,
    "n_steps": 2048,
    "gamma": 0.99,
    "eval_freq": 10000,
    "max_steps": 100,
    "data_file": "btcusd_5m.parquet",
    "final_metrics": {
        "mean_reward": 2.34,
        "final_100_mean": 3.12,
        "total_episodes": 4523
    }
}
```

### `metrics.json` - Lightweight Metrics

```json
{
    "episodes": [
        {
            "episode": 1,
            "reward": 1.23,
            "length": 100,
            "timestep": 100,
            "timestamp": "2025-01-16T14:30:25.000000"
        }
    ],
    "evaluations": [
        {
            "step": 10000,
            "mean_reward": 2.1,
            "std_reward": 0.8,
            "min_reward": 0.5,
            "max_reward": 4.2,
            "num_episodes": 5,
            "timestamp": "2025-01-16T14:35:00.000000"
        }
    ],
    "checkpoints": [
        {
            "step": 50000,
            "path": "checkpoints/model_step_50000",
            "timestamp": "2025-01-16T15:00:00.000000"
        }
    ]
}
```

### `detailed_episodes.jsonl` - Rich Episode Data

JSONL format (one JSON object per line) for efficient streaming and analysis.

```json
{
    "episode": 1,
    "reward": 2.45,
    "length": 100,
    "timestep": 100,
    "timestamp": "2025-01-16T14:30:25.000000",

    "action_counts": {
        "reject": 35,
        "approve_warning": 20,
        "approve": 45
    },
    "action_percentages": {
        "reject": 35.0,
        "approve_warning": 20.0,
        "approve": 45.0
    },

    "trade_outcomes": {
        "total_trades": 100,
        "approved_trades": 65,
        "rejected_trades": 35,
        "profitable_approved": 40,
        "losing_approved": 20,
        "liquidations": 5,
        "good_rejections": 25,
        "missed_opportunities": 10
    },

    "portfolio": {
        "starting_balance": 10000.0,
        "ending_balance": 10234.56,
        "return_pct": 2.35,
        "max_drawdown": 0.08,
        "min_health_score": 72.5,
        "final_health_score": 95.0
    },

    "reward_breakdown": {
        "pnl_rewards": 1.8,
        "rejection_rewards": 0.75,
        "health_bonuses": 0.9,
        "liquidation_penalties": -1.0
    }
}
```

---

## Loading Data with Python

### Quick Start

```python
import json
import pandas as pd
from pathlib import Path

run_dir = Path("data/training_runs/run_20250116_143022")

# Load config
with open(run_dir / "config.json") as f:
    config = json.load(f)

# Load metrics summary
with open(run_dir / "metrics.json") as f:
    metrics = json.load(f)

# Convert to DataFrames
episodes_df = pd.DataFrame(metrics["episodes"])
evals_df = pd.DataFrame(metrics["evaluations"])
```

### Loading Detailed Episodes (JSONL)

```python
import json
import pandas as pd

# Method 1: Load all at once
detailed = []
with open("data/training_runs/run_xxx/detailed_episodes.jsonl") as f:
    for line in f:
        detailed.append(json.loads(line))
detailed_df = pd.json_normalize(detailed)

# Method 2: Using pandas directly
detailed_df = pd.read_json(
    "data/training_runs/run_xxx/detailed_episodes.jsonl",
    lines=True
)

# Access nested columns (pandas flattens with dot notation)
reject_counts = detailed_df["action_counts.reject"]
portfolio_returns = detailed_df["portfolio.return_pct"]
liquidations = detailed_df["trade_outcomes.liquidations"]
```

### Analyzing Action Distribution Over Training

```python
import matplotlib.pyplot as plt

# Load detailed data
df = pd.read_json("detailed_episodes.jsonl", lines=True)

# Plot action percentages over episodes
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(df["episode"], df["action_percentages.reject"], label="Reject")
ax.plot(df["episode"], df["action_percentages.approve_warning"], label="Approve Warning")
ax.plot(df["episode"], df["action_percentages.approve"], label="Approve")
ax.set_xlabel("Episode")
ax.set_ylabel("Action %")
ax.legend()
ax.set_title("Action Distribution Over Training")
plt.savefig("action_distribution.png")
```

### Analyzing Trade Outcomes

```python
# Calculate win rate over time
df["win_rate"] = df["trade_outcomes.profitable_approved"] / df["trade_outcomes.approved_trades"]

# Calculate good rejection rate
df["good_rejection_rate"] = df["trade_outcomes.good_rejections"] / df["trade_outcomes.rejected_trades"]

# Rolling averages
df["win_rate_ma50"] = df["win_rate"].rolling(50).mean()
```

### Comparing Multiple Runs

```python
from pathlib import Path
import pandas as pd

runs_dir = Path("data/training_runs")
all_runs = []

for run_dir in runs_dir.iterdir():
    if run_dir.is_dir():
        config_path = run_dir / "config.json"
        if config_path.exists():
            with open(config_path) as f:
                config = json.load(f)
            all_runs.append({
                "run_id": config["run_id"],
                "status": config.get("status"),
                "learning_rate": config.get("learning_rate"),
                "mean_reward": config.get("final_metrics", {}).get("mean_reward"),
            })

runs_df = pd.DataFrame(all_runs)
print(runs_df.sort_values("mean_reward", ascending=False))
```

---

## Key Metrics Explained

| Metric | Description |
|--------|-------------|
| `reward` | Total episode reward (sum of step rewards) |
| `profitable_approved` | Approved trades that made money |
| `good_rejections` | Rejected trades that would have lost |
| `missed_opportunities` | Rejected trades that would have profited |
| `liquidations` | Approved trades that hit liquidation |
| `return_pct` | Episode portfolio return percentage |
| `max_drawdown` | Largest peak-to-trough decline |
| `pnl_rewards` | Reward from profitable trades |
| `rejection_rewards` | Reward from correctly rejecting bad trades |
| `health_bonuses` | Bonus for maintaining healthy portfolio |
| `liquidation_penalties` | Penalty for trades that got liquidated |

---

## TensorBoard

View training curves with TensorBoard:

```bash
uv run tensorboard --logdir data/training_runs/run_xxx/tensorboard
```

Then open http://localhost:6006 in your browser.

---

## Management Commands

```bash
# List all training runs
uv run python manage.py show_training --list

# Show specific run details
uv run python manage.py show_training --run run_20250116_143022

# Generate plots
uv run python manage.py show_training --run run_20250116_143022 --plot

# Generate markdown report
uv run python manage.py show_training --run run_20250116_143022 --report

# Compare multiple runs
uv run python manage.py show_training --compare run_1 run_2 run_3

# Delete a run
uv run python manage.py show_training --run run_20250116_143022 --delete
```
